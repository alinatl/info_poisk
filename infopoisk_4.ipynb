{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "infopoisk_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFU-bxTOTMib",
        "outputId": "1a48b36b-b8c2-47eb-d4a6-10e7d093c300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install natasha\n",
        "from natasha import (NewsNERTagger,  NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, Segmenter, Doc)\n",
        "import numpy,pymorphy2\n",
        "!pip3 install git+https://github.com/deepmipt/ner\n",
        "import ner\n",
        "extractor = ner.Extractor()"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting natasha\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/34/9abb6b5c95993001518e517f21157e2c955749ac4f3c79dc3c2cf25e72fe/natasha-1.3.0-py3-none-any.whl (34.4MB)\n",
            "\u001b[K     |████████████████████████████████| 34.4MB 119kB/s \n",
            "\u001b[?25hCollecting ipymarkup>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/9b/bf54c98d50735a4a7c84c71e92c5361730c878ebfe903d2c2d196ef66055/ipymarkup-0.9.0-py3-none-any.whl\n",
            "Collecting yargy>=0.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/07/94306844e3a5cb520660612ad98bce56c168edb596679bd541e68dfde089/yargy-0.14.0-py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.9.1)\n",
            "Collecting razdel>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n",
            "Collecting navec>=0.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/83/ad/554945ebee66fe83fefd61e043938981dd9e6136882025c506ac6faa6a4c/navec-0.9.0-py3-none-any.whl\n",
            "Collecting slovnet>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/6f/1c989335c9969421f771e4f0410ba70d82fe992ec9f3cbac9f432d8f5733/slovnet-0.4.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.8MB/s \n",
            "\u001b[?25hCollecting intervaltree>=3\n",
            "  Downloading https://files.pythonhosted.org/packages/50/fb/396d568039d21344639db96d940d40eb62befe704ef849b27949ded5c3bb/intervaltree-3.1.0.tar.gz\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from navec>=0.9.0->natasha) (1.18.5)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.2.2)\n",
            "Building wheels for collected packages: intervaltree\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26100 sha256=6c31686888cc315be901c0c94252ed309abc359101fdda0be1e667cdb2757948\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/f2/66/e9c30d3e9499e65ea2fa0d07c002e64de63bd0adaa49c445bf\n",
            "Successfully built intervaltree\n",
            "Installing collected packages: intervaltree, ipymarkup, yargy, razdel, navec, slovnet, natasha\n",
            "  Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "Successfully installed intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.3.0 navec-0.9.0 razdel-0.5.0 slovnet-0.4.0 yargy-0.14.0\n",
            "Collecting git+https://github.com/deepmipt/ner\n",
            "  Cloning https://github.com/deepmipt/ner to /tmp/pip-req-build-jg8kq_cb\n",
            "  Running command git clone -q https://github.com/deepmipt/ner /tmp/pip-req-build-jg8kq_cb\n",
            "Collecting numpy==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/e2/57c1a6af4ff0ac095dd68b12bf07771813dbf401faf1b97f5fc0cb963647/numpy-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (17.0MB)\n",
            "\u001b[K     |████████████████████████████████| 17.0MB 242kB/s \n",
            "\u001b[?25hCollecting tensorflow==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9f/57e1404fc9345759e4a732c4ab48ab4dd78fd1e60ee1270442b8850fa75f/tensorflow-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (43.5MB)\n",
            "\u001b[K     |████████████████████████████████| 43.6MB 82kB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts==2.4.393442.3710985\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 42.3MB/s \n",
            "\u001b[?25hCollecting tqdm==4.19.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/3c/341b4fa23cb3abc335207dba057c790f3bb329f6757e1fcd5d347bcf8308/tqdm-4.19.5-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
            "\u001b[?25hCollecting requests==2.18.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/df/50aa1999ab9bde74656c2919d9c0c085fd2b3775fd3eca826012bef76d8c/requests-2.18.4-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.6MB/s \n",
            "\u001b[?25hCollecting gensim==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/ed/fbbb2cc3f37a39cc4ff8e5f667374478fb852b384840aa7feb9608144290/gensim-2.3.0.tar.gz (17.2MB)\n",
            "\u001b[K     |████████████████████████████████| 17.2MB 220kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.3.0->ner==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.3.0->ner==0.0.1) (0.35.1)\n",
            "Collecting tensorflow-tensorboard<0.2.0,>=0.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/31/bb4111c3141d22bd7b2b553a26aa0c1863c86cb723919e5bd7847b3de4fc/tensorflow_tensorboard-0.1.8-py3-none-any.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.3.0->ner==0.0.1) (3.12.4)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->ner==0.0.1) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->ner==0.0.1) (0.7.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.18.4->ner==0.0.1) (3.0.4)\n",
            "Collecting urllib3<1.23,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/cb/6965947c13a94236f6d4b8223e21beb4d576dc72e8130bd7880f600839b8/urllib3-1.22-py2.py3-none-any.whl (132kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 44.2MB/s \n",
            "\u001b[?25hCollecting idna<2.7,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/cc/6dd9a3869f15c2edfab863b992838277279ce92663d334df9ecf5106f5c6/idna-2.6-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.18.4->ner==0.0.1) (2020.6.20)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==2.3.0->ner==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: smart_open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim==2.3.0->ner==0.0.1) (2.2.0)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0->ner==0.0.1) (1.0.1)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0->ner==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.3.0->tensorflow==1.3.0->ner==0.0.1) (50.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0->ner==0.0.1) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0->ner==0.0.1) (3.2.0)\n",
            "Building wheels for collected packages: ner, gensim, html5lib\n",
            "  Building wheel for ner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ner: filename=ner-0.0.1-cp36-none-any.whl size=22531 sha256=fc9eb531c5c013c9ac835f129231cde237721bd2da0853e43627d4729c141df2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p41h7xxi/wheels/46/f5/1c/0657f016f0e9725ee09f56dab547bd0bcb76fbbbc067a950ea\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gensim: filename=gensim-2.3.0-cp36-cp36m-linux_x86_64.whl size=6504643 sha256=ee0112e36cb52fbcf6ba662a14ddb6ce63883b6bac4b703a5424871ed11bf20b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/1f/86/63c886325bdffa379a7c91499bc9ea6317a4e4e0fc6e2ff1ce\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=2011212aa58b2cba23d27bd8f73f5f244678bd7859a98e262a593680ee534fb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built ner gensim html5lib\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2020.9.3 has requirement numpy>=1.15.1, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-datasets 2.1.0 has requirement requests>=2.19.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 2.3.0 has requirement requests<3,>=2.21.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.0 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scipy 1.4.1 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pywavelets 1.1.1 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 0.14.1 has requirement numpy>=1.14, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.2 has requirement numpy>=1.15.4, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas-datareader 0.9.0 has requirement requests>=2.19.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.48.0 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.0.1.post1 has requirement numpy>=1.16, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, html5lib, bleach, tensorflow-tensorboard, tensorflow, pymorphy2-dicts, pymorphy2, tqdm, urllib3, idna, requests, gensim, ner\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.2.1\n",
            "    Uninstalling bleach-3.2.1:\n",
            "      Successfully uninstalled bleach-3.2.1\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "  Found existing installation: pymorphy2 0.9.1\n",
            "    Uninstalling pymorphy2-0.9.1:\n",
            "      Successfully uninstalled pymorphy2-0.9.1\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: gensim 3.8.3\n",
            "    Uninstalling gensim-3.8.3:\n",
            "      Successfully uninstalled gensim-3.8.3\n",
            "Successfully installed bleach-1.5.0 gensim-2.3.0 html5lib-0.9999999 idna-2.6 ner-0.0.1 numpy-1.13.1 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 requests-2.18.4 tensorflow-1.3.0 tensorflow-tensorboard-0.1.8 tqdm-4.19.5 urllib3-1.22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim",
                  "idna",
                  "numpy",
                  "pymorphy2",
                  "requests",
                  "tqdm",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading from http://lnsigo.mipt.ru/export/models/ner/ner_model_total_rus.tar.gz to /usr/local/lib/python3.6/dist-packages/ner/extractor/../model/ner_model_total_rus.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 44.0M/44.3M [00:10<00:00, 4.62MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /usr/local/lib/python3.6/dist-packages/ner/extractor/../model/ner_model_total_rus.tar.gz archive into /usr/local/lib/python3.6/dist-packages/ner/extractor/../model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /usr/local/lib/python3.6/dist-packages/ner/extractor/../model/ner_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mbQrjKNdT1V",
        "outputId": "4bfc7ad1-a285-4d3e-ce7a-4fb415fe23fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from ner.corpus import Corpus\n",
        "import json\n",
        "from ner.utils import md5_hashsum, download_untar\n",
        "from glob import glob\n",
        "from ner.network import NER\n",
        "\n",
        "# Check existance of the model by hashsum\n",
        "if md5_hashsum(sorted(glob('model/*'))) != 'fd50a27b96b24cdabdda13795a3baae7':\n",
        "    # Download and extract model\n",
        "    download_url = 'http://lnsigo.mipt.ru/export/models/ner/ner_model_total_rus.tar.gz'\n",
        "    download_path = 'model/'\n",
        "    download_untar(download_url, download_path)\n",
        "\n",
        "# Load network params\n",
        "with open('model/params.json') as f:\n",
        "    network_params = json.load(f)\n",
        "\n",
        "\n",
        "c_n = Corpus(dicts_filepath='model/dict.txt')\n",
        "network =  NER(c_n, verbouse=False, pretrained_model_filepath='model/ner_model', **network_params)\n",
        "from ner.utils import tokenize, lemmatize"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading from http://lnsigo.mipt.ru/export/models/ner/ner_model_total_rus.tar.gz to model/ner_model_total_rus.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0.00/44.3M [00:00<?, ?B/s]\u001b[A\n",
            "  0%|          | 32.8k/44.3M [00:00<03:54, 188kB/s]\u001b[A\n",
            "  0%|          | 65.5k/44.3M [00:00<03:56, 187kB/s]\u001b[A\n",
            "  0%|          | 131k/44.3M [00:00<03:21, 219kB/s] \u001b[A\n",
            "  1%|          | 229k/44.3M [00:00<02:44, 267kB/s]\u001b[A\n",
            "  1%|          | 459k/44.3M [00:00<02:04, 351kB/s]\u001b[A\n",
            "  2%|▏         | 819k/44.3M [00:01<01:33, 466kB/s]\u001b[A\n",
            "  3%|▎         | 1.47M/44.3M [00:01<01:07, 632kB/s]\u001b[A\n",
            "  5%|▍         | 2.20M/44.3M [00:01<00:49, 846kB/s]\u001b[A\n",
            "  7%|▋         | 2.92M/44.3M [00:01<00:37, 1.11MB/s]\u001b[A\n",
            "  8%|▊         | 3.70M/44.3M [00:01<00:28, 1.43MB/s]\u001b[A\n",
            " 10%|█         | 4.52M/44.3M [00:01<00:22, 1.80MB/s]\u001b[A\n",
            " 12%|█▏        | 5.37M/44.3M [00:02<00:17, 2.22MB/s]\u001b[A\n",
            " 14%|█▍        | 6.23M/44.3M [00:02<00:16, 2.26MB/s]\u001b[A\n",
            " 18%|█▊        | 7.80M/44.3M [00:02<00:12, 2.91MB/s]\u001b[A\n",
            " 19%|█▉        | 8.49M/44.3M [00:02<00:11, 3.13MB/s]\u001b[A\n",
            " 21%|██        | 9.18M/44.3M [00:03<00:10, 3.31MB/s]\u001b[A\n",
            " 22%|██▏       | 9.63M/44.3M [00:03<00:15, 2.23MB/s]\u001b[A\n",
            " 23%|██▎       | 10.1M/44.3M [00:03<00:14, 2.30MB/s]\u001b[A\n",
            " 24%|██▍       | 10.6M/44.3M [00:03<00:13, 2.44MB/s]\u001b[A\n",
            " 25%|██▌       | 11.1M/44.3M [00:03<00:12, 2.56MB/s]\u001b[A\n",
            " 26%|██▋       | 11.6M/44.3M [00:04<00:12, 2.66MB/s]\u001b[A\n",
            " 27%|██▋       | 12.2M/44.3M [00:04<00:11, 2.78MB/s]\u001b[A\n",
            " 29%|██▊       | 12.7M/44.3M [00:04<00:11, 2.83MB/s]\u001b[A\n",
            " 30%|██▉       | 13.2M/44.3M [00:04<00:10, 2.90MB/s]\u001b[A\n",
            " 31%|███       | 13.8M/44.3M [00:04<00:10, 2.97MB/s]\u001b[A\n",
            " 33%|███▎      | 14.4M/44.3M [00:05<00:09, 3.03MB/s]\u001b[A\n",
            " 34%|███▍      | 14.9M/44.3M [00:05<00:09, 3.06MB/s]\u001b[A\n",
            " 35%|███▌      | 15.5M/44.3M [00:05<00:09, 3.10MB/s]\u001b[A\n",
            " 36%|███▋      | 16.1M/44.3M [00:05<00:08, 3.15MB/s]\u001b[A\n",
            " 38%|███▊      | 16.7M/44.3M [00:05<00:08, 3.18MB/s]\u001b[A\n",
            " 39%|███▉      | 17.3M/44.3M [00:05<00:08, 3.19MB/s]\u001b[A\n",
            " 40%|████      | 17.9M/44.3M [00:06<00:08, 3.22MB/s]\u001b[A\n",
            " 42%|████▏     | 18.4M/44.3M [00:06<00:07, 3.27MB/s]\u001b[A\n",
            " 43%|████▎     | 19.0M/44.3M [00:06<00:07, 3.28MB/s]\u001b[A\n",
            " 44%|████▍     | 19.6M/44.3M [00:06<00:07, 3.31MB/s]\u001b[A\n",
            " 46%|████▌     | 20.3M/44.3M [00:06<00:07, 3.29MB/s]\u001b[A\n",
            " 47%|████▋     | 20.8M/44.3M [00:06<00:07, 3.34MB/s]\u001b[A\n",
            " 48%|████▊     | 21.4M/44.3M [00:07<00:06, 3.35MB/s]\u001b[A\n",
            " 50%|████▉     | 22.1M/44.3M [00:07<00:06, 3.37MB/s]\u001b[A\n",
            " 51%|█████     | 22.6M/44.3M [00:07<00:06, 3.37MB/s]\u001b[A\n",
            " 53%|█████▎    | 23.3M/44.3M [00:07<00:06, 3.38MB/s]\u001b[A\n",
            " 54%|█████▍    | 23.9M/44.3M [00:07<00:06, 3.36MB/s]\u001b[A\n",
            " 55%|█████▌    | 24.5M/44.3M [00:08<00:05, 3.40MB/s]\u001b[A\n",
            " 57%|█████▋    | 25.1M/44.3M [00:08<00:05, 3.38MB/s]\u001b[A\n",
            " 58%|█████▊    | 25.7M/44.3M [00:08<00:05, 3.42MB/s]\u001b[A\n",
            " 59%|█████▉    | 26.3M/44.3M [00:08<00:05, 3.41MB/s]\u001b[A\n",
            " 61%|██████    | 26.9M/44.3M [00:08<00:05, 3.38MB/s]\u001b[A\n",
            " 62%|██████▏   | 27.5M/44.3M [00:08<00:04, 3.41MB/s]\u001b[A\n",
            " 64%|██████▎   | 28.1M/44.3M [00:09<00:04, 3.40MB/s]\u001b[A\n",
            " 65%|██████▍   | 28.7M/44.3M [00:09<00:04, 3.44MB/s]\u001b[A\n",
            " 66%|██████▋   | 29.4M/44.3M [00:09<00:04, 3.40MB/s]\u001b[A\n",
            " 68%|██████▊   | 30.0M/44.3M [00:09<00:04, 3.39MB/s]\u001b[A\n",
            " 69%|██████▉   | 30.6M/44.3M [00:09<00:03, 3.42MB/s]\u001b[A\n",
            " 70%|███████   | 31.2M/44.3M [00:10<00:03, 3.39MB/s]\u001b[A\n",
            " 72%|███████▏  | 31.8M/44.3M [00:10<00:03, 3.44MB/s]\u001b[A\n",
            " 73%|███████▎  | 32.4M/44.3M [00:10<00:03, 3.41MB/s]\u001b[A\n",
            " 75%|███████▍  | 33.0M/44.3M [00:10<00:03, 3.37MB/s]\u001b[A\n",
            " 76%|███████▌  | 33.6M/44.3M [00:10<00:03, 3.44MB/s]\u001b[A\n",
            " 77%|███████▋  | 34.2M/44.3M [00:10<00:02, 3.39MB/s]\u001b[A\n",
            " 79%|███████▉  | 34.9M/44.3M [00:11<00:02, 3.39MB/s]\u001b[A\n",
            " 80%|████████  | 35.5M/44.3M [00:11<00:02, 3.48MB/s]\u001b[A\n",
            " 82%|████████▏ | 36.1M/44.3M [00:11<00:02, 3.46MB/s]\u001b[A\n",
            " 83%|████████▎ | 36.7M/44.3M [00:11<00:02, 3.44MB/s]\u001b[A\n",
            " 84%|████████▍ | 37.3M/44.3M [00:11<00:02, 3.42MB/s]\u001b[A\n",
            " 86%|████████▌ | 37.9M/44.3M [00:11<00:01, 3.44MB/s]\u001b[A\n",
            " 87%|████████▋ | 38.6M/44.3M [00:12<00:01, 3.45MB/s]\u001b[A\n",
            " 89%|████████▊ | 39.2M/44.3M [00:12<00:01, 3.46MB/s]\u001b[A\n",
            " 90%|████████▉ | 39.8M/44.3M [00:12<00:01, 3.54MB/s]\u001b[A\n",
            " 91%|█████████▏| 40.4M/44.3M [00:12<00:01, 3.53MB/s]\u001b[A\n",
            " 93%|█████████▎| 41.1M/44.3M [00:12<00:00, 3.53MB/s]\u001b[A\n",
            " 94%|█████████▍| 41.7M/44.3M [00:13<00:00, 3.51MB/s]\u001b[A\n",
            " 95%|█████████▌| 42.2M/44.3M [00:13<00:00, 2.51MB/s]\u001b[A\n",
            " 97%|█████████▋| 43.1M/44.3M [00:13<00:00, 2.93MB/s]\u001b[A\n",
            " 98%|█████████▊| 43.6M/44.3M [00:13<00:00, 2.81MB/s]\u001b[A\n",
            "100%|█████████▉| 44.1M/44.3M [00:13<00:00, 2.74MB/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting model/ner_model_total_rus.tar.gz archive into model/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model/ner_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5AIKYDcfncm",
        "outputId": "ce4b65d4-e086-4559-96ff-1be2ff0a80e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "!pip install pymorphy2\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pymorphy2\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from math import log\n",
        "import collections\n",
        "import sklearn\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('russian')\n",
        "# stop_words.extend()\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.7.2)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSslBncpym2P",
        "outputId": "758e5bb2-8071-4119-a66b-29a48904fdc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZFQjtfyl0oJ",
        "outputId": "f04ec095-66d9-4452-82dd-1a6e088d2bd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip install --upgrade gensim\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "gensim.__version__"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Invalid requirement: '='\n",
            "Hint: = is not a valid operator. Did you mean == ?\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsZ5VNCfztjh",
        "outputId": "2b57b79f-9474-44cb-ca08-5f75db0d3bff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "!wget -c 'https://rusvectores.org/static/models/rusvectores4/fasttext/araneum_none_fasttextcbow_300_5_2018.tgz'\n",
        "# !unzip araneum_none_fasttextcbow_300_5_2018.tgz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-19 15:11:40--  https://rusvectores.org/static/models/rusvectores4/fasttext/araneum_none_fasttextcbow_300_5_2018.tgz\n",
            "Resolving rusvectores.org (rusvectores.org)... 116.203.104.23\n",
            "Connecting to rusvectores.org (rusvectores.org)|116.203.104.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2691248108 (2.5G) [application/x-gzip]\n",
            "Saving to: ‘araneum_none_fasttextcbow_300_5_2018.tgz’\n",
            "\n",
            "araneum_none_fastte 100%[===================>]   2.51G  19.8MB/s    in 2m 10s  \n",
            "\n",
            "2020-10-19 15:13:51 (19.8 MB/s) - ‘araneum_none_fasttextcbow_300_5_2018.tgz’ saved [2691248108/2691248108]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX7FC5VN4PcO",
        "outputId": "6ebea8a4-733c-4683-f0f2-6ae04ada6134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "# !wget -c 'https://rusvectores.org/static/models/rusvectores4/RNC/ruscorpora_upos_skipgram_300_5_2018.vec.gz'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-19 12:24:57--  https://rusvectores.org/static/models/rusvectores4/RNC/ruscorpora_upos_skipgram_300_5_2018.vec.gz\n",
            "Resolving rusvectores.org (rusvectores.org)... 116.203.104.23\n",
            "Connecting to rusvectores.org (rusvectores.org)|116.203.104.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 199864398 (191M) [application/x-gzip]\n",
            "Saving to: ‘ruscorpora_upos_skipgram_300_5_2018.vec.gz’\n",
            "\n",
            "ruscorpora_upos_ski 100%[===================>] 190.61M  27.9MB/s    in 7.5s    \n",
            "\n",
            "2020-10-19 12:25:05 (25.4 MB/s) - ‘ruscorpora_upos_skipgram_300_5_2018.vec.gz’ saved [199864398/199864398]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ-MvoFR1vlW",
        "outputId": "ca6a5e63-9c22-4a3b-970b-9b2cb14cb21c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!pip install archive\n",
        "from archive import tarfile\n",
        "\n",
        "file = \"/content/araneum_none_fasttextcbow_300_5_2018.tgz\"\n",
        "\n",
        "tar = tarfile.open(file, \"r:gz\")\n",
        "print(tar.getnames())\n",
        "tar.extractall()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: archive in /usr/local/lib/python3.6/dist-packages (0.3)\n",
            "['araneum_none_fasttextcbow_300_5_2018.model', 'araneum_none_fasttextcbow_300_5_2018.model.vectors_ngrams.npy', 'araneum_none_fasttextcbow_300_5_2018.model.vectors.npy', 'araneum_none_fasttextcbow_300_5_2018.model.vectors_vocab.npy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtweIK6kysPO"
      },
      "source": [
        "# загрузка модели\n",
        "model_file = '/content/araneum_none_fasttextcbow_300_5_2018.model'\n",
        "model = KeyedVectors.load(model_file)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpUYqDqLioAz",
        "outputId": "d07e0538-6ec2-4bf5-84a9-42c3c6a312ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#проверка наличия слова в словаре\n",
        "lemma = 'ребенок'\n",
        "lemma in model"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDqmz69s3Qsd",
        "outputId": "ba9bc2bc-6b68-4f26-ce7e-571a46d7a4c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model_file = '/content/drive/My Drive/ruscorpora_upos_skipgram_300_5_2018.vec'\n",
        "model_POS = KeyedVectors.load_word2vec_format(model_file, binary=False)\n",
        "\n",
        "#проверка наличия слова в словаре\n",
        "lemma = 'заграница_NOUN'\n",
        "lemma in model"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rpo0m3E8Vle"
      },
      "source": [
        "answers = pd.read_csv('answers_base.csv', encoding = 'windows-1251', sep = ';')\n",
        "queries = pd.read_csv('queries_base.csv', encoding = 'windows-1251', sep = ';')\n",
        "\n",
        "ans = answers[['Номер связки','Текст вопросов']].dropna(axis = 0, how ='any')\n",
        "qw = queries[['Текст вопроса', 'Номер связки\\n']].dropna(axis = 0, how ='any')\n",
        "qw.rename(columns={'Текст вопроса': 'Текст вопросов', 'Номер связки\\n': 'Номер связки'}, inplace=True)\n",
        "train = pd.concat([ans, qw.iloc[0:int(qw.shape[0]*0.7), :]]) #train\n",
        "train['idx'] = train.reset_index().index\n",
        "\n",
        "queries2 = qw.iloc[int(qw.shape[0]*0.7):, 0].tolist()\n",
        "test = dict(zip(qw.iloc[int(qw.shape[0]*0.7):, 0], qw.iloc[int(qw.shape[0]*0.7):, 1])) # test"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVrhf8dUNQxH",
        "outputId": "cb78e453-b2ac-40d7-bd66-5a686ac91714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(train.shape)\n",
        "print(len(queries2))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1646, 2)\n",
            "687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3W-AewTysnA"
      },
      "source": [
        "def create_corpus(train_text, NER = False, fun = False):\n",
        "  corpus = []\n",
        "  for question in train_text['Текст вопросов']:\n",
        "      question = question.replace('\\n', ' ').replace('/', ' ')\n",
        "      if NER == False:\n",
        "        pass\n",
        "      else:\n",
        "        question = fun(question)\n",
        "      words_doc = tokenize_ru(question)\n",
        "      corpus.append(words_doc)\n",
        "  return corpus"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv0a8ni0TKbb"
      },
      "source": [
        "def preprocess_with_natasha(text: str) -> str:\n",
        "    doc = Doc(text)\n",
        "    segmenter = Segmenter()\n",
        "    emb = NewsEmbedding()\n",
        "\n",
        "    morph_tagger = NewsMorphTagger(emb)\n",
        "    syntax_parser = NewsSyntaxParser(emb)\n",
        "    ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    doc.parse_syntax(syntax_parser)\n",
        "    doc.tag_ner(ner_tagger)\n",
        "    # display(doc.spans[1]['text'])\n",
        "    list_word_ner = [word.text for word in doc.spans]\n",
        "    for word in list_word_ner:\n",
        "        text = text.replace(word, '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def print_predict(sentence, network):\n",
        "    tokens = tokenize(sentence)\n",
        "    tokens_lemmas = lemmatize(tokens)\n",
        "    list_words_ner = []\n",
        "    tags = network.predict_for_token_batch([tokens_lemmas])[0]\n",
        "    for token, tag in zip(tokens, tags):\n",
        "        if tag != 'O':\n",
        "            list_words_ner.append(token)\n",
        "    return list_words_ner\n",
        "\n",
        "def preprocess_with_deepmipt(text: str) -> str:\n",
        "    list_words_ner = print_predict(text, network)\n",
        "    for word in list_words_ner:\n",
        "        text = text.replace(word, '')\n",
        "    return text"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_yCT7bkf9Q7"
      },
      "source": [
        "def tokenize_ru(sentence):\n",
        "    sentence = sentence.replace('\\n', ' ').replace('/', ' ')\n",
        "    sentence = re.sub(r'[\\'\"”\\,\\!\\?\\.\\-\\(\\)\\[\\]\\:\\;\\»\\«\\>\\—]', ' ', str(sentence).rstrip(\"']\"))\n",
        "    sentence = re.sub(r'[0-9]', ' ', str(sentence))\n",
        "    sentence = sentence.lower()\n",
        "    tokens = sentence.split()\n",
        "    tokens = [i for i in tokens if (i not in stop_words)]\n",
        "    tokens = [morph.parse(i)[0].normal_form for i in tokens]\n",
        "    tokens = ' '.join(tokens)\n",
        "    return tokens"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj25Z_lEgc-C"
      },
      "source": [
        "corpus = create_corpus(train)"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBITpcTH789U"
      },
      "source": [
        "# создает корпус w2v\n",
        "\n",
        "def make_w2v_doc_matr(corpus):\n",
        "    all_vectors = []\n",
        "    for doc in corpus:\n",
        "        lemmas = doc.split(' ')\n",
        "\n",
        "        lemmas_vectors = np.zeros((len(lemmas), model.vector_size))\n",
        "        vec = np.zeros((model.vector_size,))\n",
        "\n",
        "        for idx, lemma in enumerate(lemmas):\n",
        "            lemma = str(lemma)\n",
        "            if lemma in model:\n",
        "                lemmas_vectors[idx] = model[lemma]\n",
        "\n",
        "        if lemmas_vectors.shape[0] is not 0:\n",
        "            vec = np.mean(lemmas_vectors, axis=0)\n",
        "            vec = np.reshape(vec, (1, 300))\n",
        "        all_vectors.append(vec / np.sqrt(np.sum (vec ** 2)))\n",
        "        \n",
        "    matr = np.concatenate(all_vectors)\n",
        "    return matr"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdO9Bgb6goCI",
        "outputId": "24a25b0f-1ae7-4589-addd-1155f46fc24f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "matr.shape"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1646, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naE0NrT-hLvR"
      },
      "source": [
        "# векторизует запрос\n",
        "\n",
        "def make_w2v_query(doc):\n",
        "    doc = doc.replace('\\n', ' ').replace('/', ' ')\n",
        "    doc = tokenize_ru(doc)\n",
        "    lemmas = doc.split(' ')\n",
        "    lemmas_vectors = np.zeros((len(lemmas), model.vector_size))\n",
        "    vec = np.zeros((model.vector_size,))\n",
        "    for idx, lemma in enumerate(lemmas):\n",
        "        lemma = str(lemma)\n",
        "        if lemma in model:\n",
        "            lemmas_vectors[idx] = model[lemma]\n",
        "    if lemmas_vectors.shape[0] is not 0:\n",
        "        vec = np.mean(lemmas_vectors, axis=0)\n",
        "    vec = vec / np.sqrt(np.sum (vec ** 2))\n",
        "    return vec\n",
        "\n",
        "# предсказывает к какому документу относится запрос\n",
        "\n",
        "def proximity_w2v(text, matrix):\n",
        "    new_text = make_w2v_query(text)\n",
        "    proximity = []\n",
        "    for row in matr:\n",
        "        proximity.append(float(row.dot(new_text)))\n",
        "    number_sv = train['Номер связки'].tolist()\n",
        "    dict_ans = dict(zip(proximity, number_sv))\n",
        "    y_pred = dict_ans[sorted(dict_ans.keys(), reverse=True)[0]]\n",
        "    y_true = test[text]\n",
        "    return y_pred, y_true"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM1sHK3JiJlg"
      },
      "source": [
        "# считает accuracy_score\n",
        "\n",
        "def len_acc(fun, matr):\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  for text in test.keys():\n",
        "      p, t = fun(text, matr)\n",
        "      print(p, t)\n",
        "      y_true.append(t) \n",
        "      y_pred.append(p)\n",
        "  return sklearn.metrics.accuracy_score(y_true, y_pred)"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ4goBDdqTey"
      },
      "source": [
        "matr = make_w2v_doc_matr(corpus)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtZpR2yhQuMS"
      },
      "source": [
        "accuracy для матрицы из векторов документов\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRvZ_gJZutCV",
        "outputId": "c37ca8ef-ce3f-4d65-efd5-8f817c788384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len_acc(proximity_w2v, matr)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5384615384615384"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSSPiEVc4YSs"
      },
      "source": [
        "экспериментальный способ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqNsnSlR4aMy"
      },
      "source": [
        "# создает матрицу для каждого текста в отдельности\n",
        "def create_doc_matrix(text):\n",
        "    # print(text)\n",
        "    text = text.replace('\\n', ' ').replace('/', ' ')\n",
        "    text = tokenize_ru(text)\n",
        "    lemmas = text.split(' ')\n",
        "\n",
        "    lemmas_vectors = np.zeros((len(lemmas), model.vector_size))\n",
        "    vec = np.zeros((model.vector_size,))\n",
        "\n",
        "    for idx, lemma in enumerate(lemmas):\n",
        "        if lemma in model:\n",
        "            vec = model[lemma]\n",
        "            lemmas_vectors[idx] = vec / np.sqrt(np.sum (vec ** 2))\n",
        "            \n",
        "    return lemmas_vectors"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx4ResUi6N5Z"
      },
      "source": [
        "def search(query, docs, reduce_func=np.max, axis=0):\n",
        "    sims = []\n",
        "    query_m = create_doc_matrix(query)\n",
        "    for doc in docs:\n",
        "        sim = doc.dot(query_m.T)\n",
        "        sim = reduce_func(sim, axis=axis)\n",
        "        sims.append(sim.sum())\n",
        "    y_pred = int(train[train['idx'] == np.argmax(sims)]['Номер связки'])\n",
        "    y_true = int(test[query])\n",
        "    # print(y_pred, y_true)\n",
        "    return y_pred, y_true\n"
      ],
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDb7ioLZ6v8q"
      },
      "source": [
        ""
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfeZLVL54m-p",
        "outputId": "90cbbd2b-4b0c-4974-abab-7da2dc720cd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "corpus_matr = []\n",
        "for doc in corpus:\n",
        "  m = create_doc_matrix(doc)\n",
        "  corpus_matr.append(m)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzzNh17FRFw5"
      },
      "source": [
        "accuracy матрицы состоящей из матриц документов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqn71YCC6whr",
        "outputId": "c0e9319f-fc9c-4a7e-ce85-f1bcc45ccbd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len_acc(search, corpus_matr)"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4393491124260355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJpIs_szWIIq"
      },
      "source": [
        "preprocess_with_natasha "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-zY7mK-QqhQ"
      },
      "source": [
        "corpus_not_NER = create_corpus(train, True, preprocess_with_natasha)"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmAeI_WtXS__"
      },
      "source": [
        "accuracy для матрицы из векторов документов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPlLHVRe_q-k",
        "outputId": "8c451d18-1ba5-4947-9ca0-36f532f5f545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "matr = make_w2v_doc_matr(corpus_not_NER)\n",
        "len_acc(proximity_w2v, matr)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5207100591715976"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCfV22g4XFaa"
      },
      "source": [
        "accuracy матрицы состоящей из матриц документов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdGMc2-vXVfi",
        "outputId": "df1777a8-3b8b-46c3-e62d-bd466bf8d49d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "corpus_matr = []\n",
        "for doc in corpus_not_NER:\n",
        "  m = create_doc_matrix(doc)\n",
        "  corpus_matr.append(m)\n",
        "\n",
        "len_acc(search, corpus_matr)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3979289940828402"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBxq4X1QWz-s"
      },
      "source": [
        "preprocess_with_deepmipt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSnm0I0eWsOn"
      },
      "source": [
        "corpus_not_NER_p = create_corpus(train, True, preprocess_with_deepmipt)"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvAITp5MdgA5"
      },
      "source": [
        "accuracy для матрицы из векторов документов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8vdAn06M5DK",
        "outputId": "385d35af-240c-4167-cf11-8f3f25e9b341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "matr = make_w2v_doc_matr(corpus_not_NER_p)\n",
        "len_acc(proximity_w2v, matr)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5236686390532544"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 225
        }
      ]
    }
  ]
}