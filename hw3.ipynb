{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled16.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFVRH37aFU7X",
        "outputId": "722c83d7-099f-4396-fdba-02121dadbcd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "!pip install pymorphy2\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pymorphy2\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from math import log\n",
        "import collections\n",
        "import sklearn\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('russian')\n",
        "# stop_words.extend()\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "\n",
        "from ner.corpus import Corpus\n",
        "import json\n",
        "from ner.utils import md5_hashsum, download_untar\n",
        "from glob import glob\n",
        "from ner.network import NER\n",
        "\n",
        "\n",
        "# Check existance of the model by hashsum\n",
        "if md5_hashsum(sorted(glob('model/*'))) != 'fd50a27b96b24cdabdda13795a3baae7':\n",
        "    # Download and extract model\n",
        "    download_url = 'http://lnsigo.mipt.ru/export/models/ner/ner_model_total_rus.tar.gz'\n",
        "    download_path = 'model/'\n",
        "    download_untar(download_url, download_path)\n",
        "\n",
        "# Load network params\n",
        "with open('model/params.json') as f:\n",
        "    network_params = json.load(f)\n",
        "\n",
        "\n",
        "c_n = Corpus(dicts_filepath='model/dict.txt')\n",
        "network =  NER(c_n, verbouse=False, pretrained_model_filepath='model/ner_model', **network_params)\n",
        "from ner.utils import tokenize, lemmatize\n",
        "\n",
        "!pip install natasha\n",
        "from natasha import (NewsNERTagger,  NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, Segmenter, Doc)\n",
        "import pandas as pd\n",
        "# !pip3 install git+https://github.com/deepmipt/ner\n",
        "import ner"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (2.4.404381.4453942)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tid6ySBtFcl8"
      },
      "source": [
        "answers = pd.read_csv('answers_base.csv', encoding = 'windows-1251', sep = ';')\n",
        "queries = pd.read_csv('queries_base.csv', encoding = 'windows-1251', sep = ';')\n",
        "\n",
        "ans = answers[['Номер связки','Текст вопросов']].dropna(axis = 0, how ='any')\n",
        "qw = queries[['Текст вопроса', 'Номер связки\\n']].dropna(axis = 0, how ='any')\n",
        "qw.rename(columns={'Текст вопроса': 'Текст вопросов', 'Номер связки\\n': 'Номер связки'}, inplace=True)\n",
        "train = pd.concat([ans, qw.iloc[0:int(qw.shape[0]*0.7), :]]) #train\n",
        "\n",
        "queries2 = qw.iloc[int(qw.shape[0]*0.7):, 0].tolist()\n",
        "test = dict(zip(qw.iloc[int(qw.shape[0]*0.7):, 0], qw.iloc[int(qw.shape[0]*0.7):, 1])) # test"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcIgAZwZFZeF"
      },
      "source": [
        "def tokenize_ru(sentence):\n",
        "    sentence = sentence.replace('\\n', ' ').replace('/', ' ')\n",
        "    sentence = re.sub(r'[\\'\"”\\,\\!\\?\\.\\-\\(\\)\\[\\]\\:\\;\\»\\«\\>\\—]', ' ', str(sentence).rstrip(\"']\"))\n",
        "    sentence = re.sub(r'[0-9]', ' ', str(sentence))\n",
        "    sentence = sentence.lower()\n",
        "    tokens = sentence.split()\n",
        "    tokens = [i for i in tokens if (i not in stop_words)]\n",
        "    tokens = [morph.parse(i)[0].normal_form for i in tokens]\n",
        "    tokens = ' '.join(tokens)\n",
        "    return tokens"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtSJ4kwJGe2o",
        "outputId": "84cb2d99-5c1f-44e7-d8e9-f07c3a26aae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install natasha\n",
        "from natasha import (NewsNERTagger,  NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, Segmenter, Doc)\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def preprocess_with_natasha(text: str) -> str:\n",
        "    doc = Doc(text)\n",
        "    segmenter = Segmenter()\n",
        "    emb = NewsEmbedding()\n",
        "\n",
        "    morph_tagger = NewsMorphTagger(emb)\n",
        "    syntax_parser = NewsSyntaxParser(emb)\n",
        "    ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    doc.parse_syntax(syntax_parser)\n",
        "    doc.tag_ner(ner_tagger)\n",
        "    list_word_ner = [word.text for word in doc.spans]\n",
        "    print(list_word_ner)\n",
        "    for word in list_word_ner:\n",
        "        text = text.replace(word, '')\n",
        "    return text\n",
        "\n",
        "def print_predict(sentence, network):\n",
        "    tokens = tokenize(sentence)\n",
        "    tokens_lemmas = lemmatize(tokens)\n",
        "    list_words_ner = []\n",
        "    tags = network.predict_for_token_batch([tokens_lemmas])[0]\n",
        "    for token, tag in zip(tokens, tags):\n",
        "        if tag != 'O':\n",
        "            list_words_ner.append(token)\n",
        "    return list_words_ner\n",
        "\n",
        "def preprocess_with_deepmipt(text: str) -> str:\n",
        "    list_words_ner = print_predict(text, network)\n",
        "    for word in list_words_ner:\n",
        "        text = text.replace(word, '')\n",
        "    return text"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting natasha\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/34/9abb6b5c95993001518e517f21157e2c955749ac4f3c79dc3c2cf25e72fe/natasha-1.3.0-py3-none-any.whl (34.4MB)\n",
            "\u001b[K     |████████████████████████████████| 34.4MB 83kB/s \n",
            "\u001b[?25hCollecting slovnet>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/6f/1c989335c9969421f771e4f0410ba70d82fe992ec9f3cbac9f432d8f5733/slovnet-0.4.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.9.1)\n",
            "Collecting navec>=0.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/83/ad/554945ebee66fe83fefd61e043938981dd9e6136882025c506ac6faa6a4c/navec-0.9.0-py3-none-any.whl\n",
            "Collecting razdel>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n",
            "Collecting ipymarkup>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/9b/bf54c98d50735a4a7c84c71e92c5361730c878ebfe903d2c2d196ef66055/ipymarkup-0.9.0-py3-none-any.whl\n",
            "Collecting yargy>=0.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/07/94306844e3a5cb520660612ad98bce56c168edb596679bd541e68dfde089/yargy-0.14.0-py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from slovnet>=0.3.0->natasha) (1.18.5)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (2.4.404381.4453942)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading https://files.pythonhosted.org/packages/50/fb/396d568039d21344639db96d940d40eb62befe704ef849b27949ded5c3bb/intervaltree-3.1.0.tar.gz\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.2.2)\n",
            "Building wheels for collected packages: intervaltree\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26100 sha256=648cd0077ed2a4b62afe49fcabe8dcee195ecccb3278671300c3a995024a4e2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/f2/66/e9c30d3e9499e65ea2fa0d07c002e64de63bd0adaa49c445bf\n",
            "Successfully built intervaltree\n",
            "Installing collected packages: razdel, navec, slovnet, intervaltree, ipymarkup, yargy, natasha\n",
            "  Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "Successfully installed intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.3.0 navec-0.9.0 razdel-0.5.0 slovnet-0.4.0 yargy-0.14.0\n",
            "Collecting git+https://github.com/deepmipt/ner\n",
            "  Cloning https://github.com/deepmipt/ner to /tmp/pip-req-build-d0o510ki\n",
            "  Running command git clone -q https://github.com/deepmipt/ner /tmp/pip-req-build-d0o510ki\n",
            "Collecting numpy==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/e2/57c1a6af4ff0ac095dd68b12bf07771813dbf401faf1b97f5fc0cb963647/numpy-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (17.0MB)\n",
            "\u001b[K     |████████████████████████████████| 17.0MB 200kB/s \n",
            "\u001b[?25hCollecting tensorflow==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9f/57e1404fc9345759e4a732c4ab48ab4dd78fd1e60ee1270442b8850fa75f/tensorflow-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (43.5MB)\n",
            "\u001b[K     |████████████████████████████████| 43.6MB 70kB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts==2.4.393442.3710985\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 8.7MB/s \n",
            "\u001b[?25hCollecting tqdm==4.19.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/3c/341b4fa23cb3abc335207dba057c790f3bb329f6757e1fcd5d347bcf8308/tqdm-4.19.5-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.0MB/s \n",
            "\u001b[?25hCollecting requests==2.18.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/df/50aa1999ab9bde74656c2919d9c0c085fd2b3775fd3eca826012bef76d8c/requests-2.18.4-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.7MB/s \n",
            "\u001b[?25hCollecting gensim==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/ed/fbbb2cc3f37a39cc4ff8e5f667374478fb852b384840aa7feb9608144290/gensim-2.3.0.tar.gz (17.2MB)\n",
            "\u001b[K     |████████████████████████████████| 17.2MB 206kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.3.0->ner==0.0.1) (0.35.1)\n",
            "Requirement already satisfied: protobuf>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.3.0->ner==0.0.1) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.3.0->ner==0.0.1) (1.15.0)\n",
            "Collecting tensorflow-tensorboard<0.2.0,>=0.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/31/bb4111c3141d22bd7b2b553a26aa0c1863c86cb723919e5bd7847b3de4fc/tensorflow_tensorboard-0.1.8-py3-none-any.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 30.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->ner==0.0.1) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->ner==0.0.1) (0.7.2)\n",
            "Collecting urllib3<1.23,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/cb/6965947c13a94236f6d4b8223e21beb4d576dc72e8130bd7880f600839b8/urllib3-1.22-py2.py3-none-any.whl (132kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 49.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.18.4->ner==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.18.4->ner==0.0.1) (2020.6.20)\n",
            "Collecting idna<2.7,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/cc/6dd9a3869f15c2edfab863b992838277279ce92663d334df9ecf5106f5c6/idna-2.6-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==2.3.0->ner==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: smart_open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim==2.3.0->ner==0.0.1) (2.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.3.0->tensorflow==1.3.0->ner==0.0.1) (50.3.0)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.1MB/s \n",
            "\u001b[?25hCollecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0->ner==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0->ner==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0->ner==0.0.1) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0->ner==0.0.1) (3.2.0)\n",
            "Building wheels for collected packages: ner, gensim, html5lib\n",
            "  Building wheel for ner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ner: filename=ner-0.0.1-cp36-none-any.whl size=22531 sha256=978d8071921b1e587e7d9dbb84c3a9675928a71297d7650896fe0a53a1e96df7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a1hw50rq/wheels/46/f5/1c/0657f016f0e9725ee09f56dab547bd0bcb76fbbbc067a950ea\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gensim: filename=gensim-2.3.0-cp36-cp36m-linux_x86_64.whl size=6504638 sha256=cef2201cb616e37a27b879525f0488e0eed79ee61e901c42d79f79077ed74862\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/1f/86/63c886325bdffa379a7c91499bc9ea6317a4e4e0fc6e2ff1ce\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=e2c385889a59b7481e50696b353a50965689b904cc5cfb316e03a6828ed5a061\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built ner gensim html5lib\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2020.9.3 has requirement numpy>=1.15.1, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-datasets 2.1.0 has requirement requests>=2.19.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 2.3.0 has requirement requests<3,>=2.21.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.0 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scipy 1.4.1 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pywavelets 1.1.1 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 0.14.1 has requirement numpy>=1.14, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.2 has requirement numpy>=1.15.4, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas-datareader 0.9.0 has requirement requests>=2.19.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.48.0 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.0.1.post1 has requirement numpy>=1.16, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, html5lib, bleach, tensorflow-tensorboard, tensorflow, pymorphy2-dicts, pymorphy2, tqdm, urllib3, idna, requests, gensim, ner\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.2.1\n",
            "    Uninstalling bleach-3.2.1:\n",
            "      Successfully uninstalled bleach-3.2.1\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "  Found existing installation: pymorphy2 0.9.1\n",
            "    Uninstalling pymorphy2-0.9.1:\n",
            "      Successfully uninstalled pymorphy2-0.9.1\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed bleach-1.5.0 gensim-2.3.0 html5lib-0.9999999 idna-2.6 ner-0.0.1 numpy-1.13.1 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 requests-2.18.4 tensorflow-1.3.0 tensorflow-tensorboard-0.1.8 tqdm-4.19.5 urllib3-1.22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pymorphy2"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading from http://lnsigo.mipt.ru/export/models/ner/ner_model_total_rus.tar.gz to /usr/local/lib/python3.6/dist-packages/ner/extractor/../model/ner_model_total_rus.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 44.3M/44.3M [00:09<00:00, 4.82MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /usr/local/lib/python3.6/dist-packages/ner/extractor/../model/ner_model_total_rus.tar.gz archive into /usr/local/lib/python3.6/dist-packages/ner/extractor/../model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /usr/local/lib/python3.6/dist-packages/ner/extractor/../model/ner_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaUNJrj5GfS8"
      },
      "source": [
        "def create_corpus(train_text, NER = False):\n",
        "  corpus = []\n",
        "  for question in train_text['Текст вопросов']:\n",
        "      question = question.replace('\\n', ' ').replace('/', ' ')\n",
        "      if NER == False:\n",
        "        pass\n",
        "      else:\n",
        "        question = preprocess_with_deepmipt(question)\n",
        "      words_doc = tokenize_ru(question)\n",
        "      corpus.append(words_doc)\n",
        "  return corpus"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y51G26-CGlzg"
      },
      "source": [
        "def create_mat_tfidf(corpus):\n",
        "  X = vectorizer.fit_transform(corpus)\n",
        "  X.toarray()\n",
        "  return X\n",
        "\n",
        "def bm25(tf_q_d, l, N, nq, corpus) -> float:\n",
        "    k = 2.0\n",
        "    b = 0.75\n",
        "    aver = Average(corpus)\n",
        "    TF = (tf_q_d * (k+1))/(tf_q_d + k*(1 - b + b*(l/aver)))\n",
        "    IDF = log((N-nq+0.5)/(nq + 0.5))\n",
        "    result = IDF*TF\n",
        "    return result\n",
        "\n",
        "def Average(lst):\n",
        "    lst2 = []\n",
        "    d = {}\n",
        "    for doc in lst:\n",
        "        lst2.append(len(doc.split(' ')))\n",
        "    return sum(lst2) / len(lst)\n",
        "\n",
        "def create_mat_bm25(corpus):\n",
        "  N = len(corpus)\n",
        "  nq = Counter(full_data)\n",
        "  matr = np.zeros((N, len(nq)))\n",
        "\n",
        "  for i, doc in enumerate(corpus):\n",
        "      doc = doc.split(' ')\n",
        "      tf_q_d = Counter(doc)\n",
        "      l = len(doc)\n",
        "      for j, word in enumerate(set(full_data)):\n",
        "          if tf_q_d[word] == 0:\n",
        "              matr[i, j] = 0\n",
        "          else:\n",
        "              matr[i, j] = bm25(tf_q_d[word], l, N, nq[word], corpus)\n",
        "  return matr\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qaj9H19zG_Fi"
      },
      "source": [
        "# перевод запросов в соответствующий вид\n",
        "\n",
        "\n",
        "def to_tiidf(query):\n",
        "    query = query.replace('\\n', ' ').replace('/', ' ')\n",
        "    query = tokenize_ru(query)\n",
        "    # print(query)\n",
        "    vect = vectorizer.transform([query]).toarray()\n",
        "    return vect \n",
        "\n",
        "def query_f_tfidf(text, matrix):\n",
        "    # ner_text = preprocess_with_natasha(text)\n",
        "    new_doc = to_tiidf(text).T\n",
        "    proximity = []\n",
        "    for row in matrix:\n",
        "        proximity.append(float(row.dot(new_doc)))\n",
        "    number_sv = train['Номер связки'].tolist()\n",
        "    dict_ans = dict(zip(proximity, number_sv))\n",
        "    y_pred = dict_ans[sorted(dict_ans.keys(), reverse=True)[0]]\n",
        "    y_true = test[text]\n",
        "    return y_pred, y_true\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def to_bm25(query, full_data):\n",
        "    vect = np.zeros((1, len(full_data)))\n",
        "    query = query.replace('\\n', ' ').replace('/', ' ')\n",
        "    query = tokenize_ru(query)\n",
        "    for word in query.split(' '):\n",
        "        if word in full_data:\n",
        "            vect[0, full_data.index(word)] = 1\n",
        "    return vect\n",
        "\n",
        "def query_f_bm25(text, matrix):\n",
        "    # ner_text = preprocess_with_natasha(text)\n",
        "    new_doc = to_bm25(text, list(set(full_data))).T\n",
        "    proximity = []\n",
        "    for row in matrix:\n",
        "        r = row.reshape(1, row.shape[0])\n",
        "        proximity.append(float(r.dot(new_doc)))\n",
        "    number_sv = train['Номер связки'].tolist()\n",
        "    dict_ans = dict(zip(proximity, number_sv))\n",
        "\n",
        "    y_pred = dict_ans[sorted(dict_ans.keys(), reverse=True)[0]]\n",
        "    y_true = test[text]\n",
        "    return y_pred, y_true"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYxmGJnNa9Rl"
      },
      "source": [
        "def len_acc(fun, matr):\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  for text in test.keys():\n",
        "      p, t = fun(text, matr)\n",
        "      y_true.append(t) \n",
        "      y_pred.append(p)\n",
        "  return sklearn.metrics.accuracy_score(y_true, y_pred)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqv6bOfCbcQa"
      },
      "source": [
        "corpus = create_corpus(train) # создаю корпус для текстов с неудаленными NER"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfHhm0zUhKY9"
      },
      "source": [
        "это accuracy для tf-idf c ner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfMapI75c7P2",
        "outputId": "b4fca9c5-d310-4ad1-9499-7fa9dded81d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = create_mat_tfidf(corpus)\n",
        "full_data = ' '.join(corpus).split(' ')\n",
        "len_acc(query_f_tfidf, X)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35798816568047337"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNrxIVAPg5Ek"
      },
      "source": [
        "это accuracy для bm25 c ner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K7paTFnffTG",
        "outputId": "7f0951e3-76c5-4e8f-e206-cf3c80a8eecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X1 = create_mat_bm25(corpus)\n",
        "full_data = ' '.join(corpus).split(' ')\n",
        "len_acc(query_f_bm25, X1)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5207100591715976"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJpFrSGLjfR9"
      },
      "source": [
        "это accuracy для tf-idf без ner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETSGqu9qiiwJ"
      },
      "source": [
        "corpus_not_NER = create_corpus(train, True) # создаю корпус для текстов с удаленными NER"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUbHlzdZjAuD",
        "outputId": "301f2f0a-d02d-4dbb-dd6d-7019b7a02c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# X_not_NER = create_mat_tfidf(corpus_not_NER)\n",
        "# full_data = ' '.join(corpus_not_NER).split(' ')\n",
        "len_acc(query_f_tfidf, X_not_NER)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2869822485207101"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QkNnK5Rjt9q"
      },
      "source": [
        "это accuracy для bm25 без ner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnQof_th2KSF",
        "outputId": "a95bea57-4cd1-4a92-ee2b-28255577ae26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_not_NER1 = create_mat_bm25(corpus_not_NER)\n",
        "full_data = ' '.join(corpus_not_NER).split(' ')\n",
        "len_acc(query_f_bm25, X_not_NER1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4985207100591716"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYrVOQspjvmM"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fboyFNmguBxi"
      },
      "source": [
        ""
      ],
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXIukZTT4fWR"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}